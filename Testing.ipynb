{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pulp as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class karmbandit:\n",
    "    \"\"\"This is the k arm bandit problem\n",
    "\n",
    "    Attributes:\n",
    "        d: number of arms\n",
    "        distribution (str): distribution of rewards\n",
    "        params (array): parameters of the distribution, the line i contain the parameters of the distribution of the arm i\n",
    "        best_arm (int): index of the best arm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d, distribution, params):\n",
    "        \"\"\"Init the k arm bandit problem\n",
    "\n",
    "        Args:\n",
    "            d (int): number of arms\n",
    "            distribution (str): distribution of rewards\n",
    "            params (array): parameters of the distribution, the line i contain the parameters of the distribution of the arm i\n",
    "        \"\"\"\n",
    "\n",
    "        self.d = d\n",
    "        self.distribution = distribution\n",
    "        self.params = params\n",
    "        if distribution == 'bernoulli':\n",
    "            self.mus = params\n",
    "        if distribution == 'gaussian':\n",
    "            self.mus = params[:,0]\n",
    "            self.sigmas = params[:,1]\n",
    "        \n",
    "        self.best_arm = np.argmax(self.mus)\n",
    "\n",
    "    def pull(self):\n",
    "        \"\"\"Pull the arms\n",
    "        \n",
    "        Returns:\n",
    "            reward (float): reward of the arms\n",
    "        \"\"\"\n",
    "\n",
    "        if self.distribution == 'bernoulli':\n",
    "            return np.random.binomial(1,self.mus)\n",
    "        if self.distribution == 'gaussian':\n",
    "            return np.random.normal(self.mus, self.sigmas)\n",
    "\n",
    "class karmpolicy:\n",
    "    \"\"\"This is the k arm bandit policy\n",
    "\n",
    "    Attributes:\n",
    "        d: number of armsArgs:\n",
    "            index (int): index of the arm to pull\n",
    "        self.muhat (array): empiric mean of the arms\n",
    "        t (int): time step\n",
    "        w (array): number of time an arm is played\n",
    "        regrets (list) : regret at each time step\n",
    "        policy (str): policy to use\n",
    "        params (array): parameters of the policy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, karmbandit, policy, algoparams = None, prior = \"uniform\"):\n",
    "        \"\"\"Init the k arm bandit policy\n",
    "\n",
    "        Args:\n",
    "            d (int): number of arms\n",
    "            policy (str): policy to use\n",
    "            algoparams (array): parameters of the policy, c for ucb, epsilon for epsilon-greedy, param of the prior for thompson sampling\n",
    "            prior (str): by default it is the uniform prior\n",
    "        \"\"\"\n",
    "        # get the parameters of the k arm bandit problem\n",
    "        self.karmbandit = karmbandit\n",
    "        self.d = karmbandit.d\n",
    "        self.bestarm = karmbandit.best_arm\n",
    "        self.mus = karmbandit.mu\n",
    "        self.sigmas = karmbandit.sigma\n",
    "        self.distribution = karmbandit.distribution\n",
    "        self.mustar = karmbandit.best_arm\n",
    "\n",
    "        # store in the class the policy and the initial parameters\n",
    "        self.policy = policy\n",
    "        self.algoparams = algoparams\n",
    "\n",
    "        # initialise the paramater of the algorithm\n",
    "        self.regrets = []\n",
    "        self.muhats = np.zeros(self.d)\n",
    "        # small hack to avoid division by 0 before the arm is played for the first time\n",
    "        self.t = 1\n",
    "        self.w = np.ones(self.d) * 10**(-8)\n",
    "\n",
    "        if policy == 'epsilon-greedy':\n",
    "            self.epsilon = self.algoparams[0].copy()\n",
    "        if policy == 'ucb':\n",
    "            self.c = self.algoparams[0].copy()\n",
    "        if policy == 'thompson-sampling':\n",
    "            if self.distribution == 'bernoulli':\n",
    "                if prior == \"beta\":\n",
    "                    self.alphas = self.algoparams[:0].copy()\n",
    "                    self.betas = self.algoparams[:1].copy()\n",
    "                if prior == \"uniform\":\n",
    "                    self.alphas = np.ones(self.d)\n",
    "                    self.betas = np.ones(self.d)\n",
    "                if prior == \"gaussian\":\n",
    "                    # In the case of a gaussian prior for bernoulli distribution we assume that the prior is uniform on R\n",
    "                    # And that the bernoulli are gaussian with variance 1/4 \n",
    "                    self.mus = np.zeros(self.d)\n",
    "                    self.sigmapost = 10**-8 * np.ones(self.d)\n",
    "            if self.distribution == 'gaussian':\n",
    "                if prior == \"gaussian\":\n",
    "                    self.mushats = self.algoparams[:0].copy()\n",
    "                    self.sigmapost = self.algoparams[:1].copy()\n",
    "                if prior == \"uniform\":\n",
    "                    self.mushats = np.zeros(self.d)\n",
    "                    self.sigmapost = 10**8 * np.ones(self.d)\n",
    "        if policy == \"klucb\":\n",
    "            pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # initialise the paramater of the algorithm\n",
    "        self.regrets = []\n",
    "        self.muhats = np.zeros(self.d)\n",
    "        # small hack to avoid division by 0 before the arm is played for the first time\n",
    "        self.t = 1\n",
    "        self.w = np.ones(self.d) * 10**(-8)\n",
    "\n",
    "        if self.policy == 'epsilon-greedy':\n",
    "            self.epsilon = self.algoparams[0]\n",
    "        if self.policy == 'ucb':\n",
    "            self.c = self.algoparams[0]\n",
    "        if self.policy == 'thompson-sampling':\n",
    "            if self.distribution == 'bernoulli':\n",
    "                if self.prior == \"beta\":\n",
    "                    self.alphas = self.algoparams[:0].copy()\n",
    "                    self.betas = self.algoparams[:1].copy()\n",
    "                if self.prior == \"uniform\":\n",
    "                    self.alphas = np.ones(self.d)\n",
    "                    self.betas = np.ones(self.d)\n",
    "                if self.prior == \"gaussian\":\n",
    "                    # In the case of a gaussian prior for bernoulli distribution we assume that the prior is uniform on R\n",
    "                    # And that the bernoulli are gaussian with variance 1/4 \n",
    "                    self.mus = np.zeros(self.d)\n",
    "                    self.sigmapost = 10**-8 * np.ones(self.d)\n",
    "            if self.distribution == 'gaussian':\n",
    "                if self.prior == \"gaussian\":\n",
    "                    self.mushats = self.algoparams[:0].copy()\n",
    "                    self.sigmapost = self.algoparams[:1].copy()\n",
    "                if self.prior == \"uniform\":\n",
    "                    self.mushats = np.zeros(self.d)\n",
    "                    self.sigmapost = 10**8 * np.ones(self.d)\n",
    "        if self.policy == \"klucb\":\n",
    "            pass\n",
    "    \n",
    "\n",
    "\n",
    "    def select(self):\n",
    "        \"\"\"Select the arm to pull according to the algorithm policy\n",
    "        \n",
    "        Returns:\n",
    "            index (int): index of the arm to pull\n",
    "        \"\"\"\n",
    "        if self.policy == 'ucb':\n",
    "            index = np.argmax(self.muhat + self.c * np.sqrt(np.log(self.t) / (2 * self.w)))\n",
    "        if self.policy == 'epsilon-greedy':\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                index = np.random.randint(self.d)\n",
    "            else:\n",
    "                index = np.argmax(self.muhats)\n",
    "        if self.policy == 'thompson-sampling':\n",
    "            if self.prior == 'bernoulli' or self.prior == 'uniform':\n",
    "                index = np.argmax(np.random.beta(self.alphas, self.betas))\n",
    "            if self.prior == 'gaussian':\n",
    "                index = np.argmax(np.random.normal(self.muhats, self.sigmapost))\n",
    "\n",
    "        if self.policy == \"klucb\":\n",
    "            pass\n",
    "        return index\n",
    "\n",
    "    def update(self, index, reward):\n",
    "        \"\"\"Update the policy\n",
    "        \n",
    "        Args:\n",
    "            index (int): index of the arm to pull\n",
    "            reward (float): reward of the arm\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "\n",
    "        \n",
    "        if self.w[index] < 0.5:\n",
    "            # correct the hack to avoid division by 0\n",
    "            self.w[index] = 1\n",
    "        else:\n",
    "            self.w[index] += 1\n",
    "\n",
    "        if self.policy == 'thompson-sampling':\n",
    "            if self.distribution == 'bernoulli':\n",
    "                if self.prior == 'beta' or self.prior == 'uniform':\n",
    "                    self.alphas[index] += reward\n",
    "                    self.betas[index] += 1 - reward\n",
    "                if self.prior == 'gaussian':\n",
    "                    self.muhats[index] = (self.muhats[index] * (self.w[index] - 1) + reward) / self.w[index] \n",
    "                    # I already verified the above formula\n",
    "                    if self.sigmapost[index] == 10**8:\n",
    "                        self.sigmapost[index] = np.sqrt(1/4)\n",
    "                    else:\n",
    "                        self.sigmapost[index] = np.sqrt((1/self.sigmapost[index]**2 + 1/4)**-1)\n",
    "                    \n",
    "\n",
    "            if self.distribution == 'gaussian':\n",
    "                if self.prior == 'uniform':\n",
    "                    self.muhats[index] = (self.muhats[index] * (self.w[index] - 1) + reward) / self.w[index]\n",
    "                    if self.sigmapost[index] == 10**8:\n",
    "                        self.sigmapost[index] = self.sigmas[index]\n",
    "                    else:\n",
    "                        self.sigmapost[index] = np.sqrt((1/self.sigmapost[index]**2 + 1/self.sigmas[index]**2)**-1)\n",
    "                if self.prior == 'gaussian':\n",
    "                    self.sigmapost[index] = np.sqrt((1/self.sigmapost[index] + 1/self.sigmas[index])**-1)\n",
    "                    self.muhats = self.sigmapost[index]**2 * (self.muhats[index] / self.sigmapost[index]**2 + reward / self.sigmas[index]**2)\n",
    "                    # The above formula is verified\n",
    "\n",
    "\n",
    "        if self.policy == \"ucb\":\n",
    "            self.muhats[index] = (self.muhats[index] * (self.w[index] - 1) + reward) / self.w[index]\n",
    "\n",
    "        self.regrets.append(self.mustar - self.mus[index])\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'karmbandit' object has no attribute 'mu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# test the code\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# create a d arm bandit problem\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m problem \u001b[39m=\u001b[39m karmbandit(d\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, distribution\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbernoulli\u001b[39;49m\u001b[39m'\u001b[39;49m, params\u001b[39m=\u001b[39;49m[\u001b[39m0.1\u001b[39;49m, \u001b[39m0.2\u001b[39;49m, \u001b[39m0.3\u001b[39;49m, \u001b[39m0.4\u001b[39;49m, \u001b[39m0.5\u001b[39;49m, \u001b[39m0.6\u001b[39;49m, \u001b[39m0.7\u001b[39;49m, \u001b[39m0.8\u001b[39;49m, \u001b[39m0.9\u001b[39;49m, \u001b[39m1\u001b[39;49m])\n\u001b[1;32m      6\u001b[0m \u001b[39m# initialise the algorithms\u001b[39;00m\n\u001b[1;32m      7\u001b[0m TS \u001b[39m=\u001b[39m karmpolicy(problem, \u001b[39m\"\u001b[39m\u001b[39mthompson-sampling\u001b[39m\u001b[39m\"\u001b[39m, prior \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m, in \u001b[0;36mkarmbandit.__init__\u001b[0;34m(self, d, distribution, params)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmus \u001b[39m=\u001b[39m params[:,\u001b[39m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas \u001b[39m=\u001b[39m params[:,\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_arm \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmu)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'karmbandit' object has no attribute 'mu'"
     ]
    }
   ],
   "source": [
    "# test the code\n",
    "\n",
    "# create a d arm bandit problem\n",
    "problem = karmbandit(d=10, distribution='bernoulli', params=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "\n",
    "# initialise the algorithms\n",
    "TS = karmpolicy(problem, \"thompson-sampling\", prior = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpy.broadcast at 0x181d650>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast([1],[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
