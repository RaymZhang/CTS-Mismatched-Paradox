{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pulp as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "class karmbandit:\n",
    "    \"\"\"This is the k arm bandit problem\n",
    "\n",
    "    Attributes:\n",
    "        d: number of arms\n",
    "        distribution (str): distribution of rewards\n",
    "        params (array): parameters of the distribution, the line i contain the parameters of the distribution of the arm i\n",
    "        best_arm (int): index of the best arm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d, distribution, params):\n",
    "        \"\"\"Init the k arm bandit problem\n",
    "\n",
    "        Args:\n",
    "            d (int): number of arms\n",
    "            distribution (str): distribution of rewards\n",
    "            params (array): parameters of the distribution, the line i contain the parameters of the distribution of the arm i\n",
    "        \"\"\"\n",
    "\n",
    "        self.d = d\n",
    "        self.distribution = distribution\n",
    "        if distribution == 'bernoulli':\n",
    "            self.mu = params\n",
    "        if distribution == 'gaussian':\n",
    "            self.mu = params[:,0]\n",
    "            self.sigma = params[:,1]\n",
    "        \n",
    "        self.best_arm = np.argmax(self.mu)\n",
    "\n",
    "    def pull(self, index):\n",
    "        \"\"\"Pull the arm index\n",
    "        \n",
    "        Args:\n",
    "            index (int): index of the arm to pull\n",
    "            \n",
    "        Returns:\n",
    "            reward (float): reward of the arm\n",
    "        \"\"\"\n",
    "\n",
    "        if self.distribution == 'bernoulli':\n",
    "            return np.random.binomial(1, self.mu[index])\n",
    "        if self.distribution == 'gaussian':\n",
    "            return np.random.normal(self.mu[index], self.sigma[index])\n",
    "\n",
    "class karmpolicy:\n",
    "    \"\"\"This is the k arm bandit policy\n",
    "\n",
    "    Attributes:\n",
    "        d: number of arms\n",
    "        self.muhat (array): empiric mean of the arms\n",
    "        t (int): time step\n",
    "        w (array): number of time an arm is played\n",
    "        regrets (list) : regret at each time step\n",
    "        policy (str): policy to use\n",
    "        params (array): parameters of the policy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, karmbandit, policy, params, distribution, prior = \"uniform\"):\n",
    "        \"\"\"Init the k arm bandit policy\n",
    "\n",
    "        Args:\n",
    "            d (int): number of arms\n",
    "            policy (str): policy to use\n",
    "            params (array): parameters of the policy\n",
    "        \"\"\"\n",
    "\n",
    "        self.d = karmbandit.d\n",
    "        self.bestarm = karmbandit.best_arm\n",
    "        self.mus = karmbandit.mu\n",
    "        self.mustar = karmbandit.mu[self.bestarm]\n",
    "\n",
    "        self.policy = policy\n",
    "        self.regrets = []\n",
    "        self.muhats = np.zeros(self.d)\n",
    "        self.t = 0\n",
    "        self.w = np.zeros(self.d)\n",
    "\n",
    "        if policy == 'epsilon-greedy':\n",
    "            self.epsilon = params[0]\n",
    "        if policy == 'ucb':\n",
    "            self.c = params[0]\n",
    "        if policy == 'thompson-sampling':\n",
    "            if distribution == 'bernoulli':\n",
    "                if prior == \"beta\":\n",
    "                    self.alphas = params[:0]\n",
    "                    self.betas = params[:1]\n",
    "                if prior == \"uniform\":\n",
    "                    self.alphas = np.ones(self.d)\n",
    "                    self.betas = np.ones(self.d)\n",
    "                if prior == \"gaussian\":\n",
    "                    self.mus = params[:0]\n",
    "                    self.sigmahats = params[:1]\n",
    "            if distribution == 'gaussian':\n",
    "                if prior == \"gaussian\":\n",
    "                    self.mushats = params[:0]\n",
    "                    self.sigmahats = params[:1]\n",
    "        if policy == \"klucb\":\n",
    "            pass\n",
    "\n",
    "    def select(self):\n",
    "        \"\"\"Select the arm to pull according to the algorithm policy\n",
    "        \n",
    "        Returns:\n",
    "            index (int): index of the arm to pull\n",
    "        \"\"\"\n",
    "        if self.policy == 'ucb':\n",
    "            index = np.argmax(self.muhat + self.c * np.sqrt(np.log(self.t) / (2 * self.w)))\n",
    "        if self.policy == 'epsilon-greedy':\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                index = np.random.randint(self.d)\n",
    "            else:\n",
    "                index = np.argmax(self.muhats)\n",
    "        if self.policy == 'thompson-sampling':\n",
    "            if self.prior == 'bernoulli' or self.prior == 'uniform':\n",
    "                index = np.argmax(np.random.beta(self.alphas, self.betas))\n",
    "            if self.prior == 'gaussian':\n",
    "                index = np.argmax(np.random.normal(self.muhats, self.sigmahats))\n",
    "\n",
    "        if self.policy == \"klucb\":\n",
    "            pass\n",
    "        return index\n",
    "\n",
    "    def update(self, index, reward):\n",
    "        \"\"\"Update the policy\n",
    "        \n",
    "        Args:\n",
    "            index (int): index of the arm to pull\n",
    "            reward (float): reward of the arm\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        self.w[index] += 1\n",
    "\n",
    "        if self.policy == 'thompson-sampling':\n",
    "            if self.distribution == 'bernoulli':\n",
    "                self.alphas[index] += reward\n",
    "                self.betas[index] += 1 - reward\n",
    "            if self.distribution == 'gaussian':\n",
    "                self.muhats[index] = (self.muhats[index] * (self.w[index] - 1) + reward) / self.w[index]\n",
    "                self.sigmahats[index] += 1\n",
    "        \n",
    "        if self.policy == \"ucb\":\n",
    "            self.muhats[index] = (self.muhats[index] * (self.w[index] - 1) + reward) / self.w[index]\n",
    "\n",
    "        self.regrets.append(self.mustar - self.mus[index])\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
