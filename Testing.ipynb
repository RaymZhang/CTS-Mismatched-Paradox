{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pulp as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class karmbandit:\n",
    "    \"\"\"This is the k arm bandit problem\n",
    "\n",
    "    Attributes:\n",
    "        d: number of arms\n",
    "        distribution (str): distribution of rewards\n",
    "        params (array): parameters of the distribution, the line i contain the parameters of the distribution of the arm i\n",
    "        best_arm (int): index of the best arm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d, distribution, params):\n",
    "        \"\"\"Init the k arm bandit problem\n",
    "\n",
    "        Args:\n",
    "            d (int): number of arms\n",
    "            distribution (str): distribution of rewards\n",
    "            params (array): parameters of the distribution, the line i contain the parameters of the distribution of the arm i\n",
    "        \"\"\"\n",
    "\n",
    "        self.d = d\n",
    "        self.distribution = distribution\n",
    "        self.params = params\n",
    "        if distribution == 'bernoulli':\n",
    "            self.mus = params\n",
    "            self.sigmas = 1/2*np.ones(d)\n",
    "        if distribution == 'gaussian':\n",
    "            self.mus = params[:,0]\n",
    "            self.sigmas = params[:,1]\n",
    "        \n",
    "        self.best_arm = np.argmax(self.mus)\n",
    "        self.mustar = self.mus[self.best_arm]\n",
    "\n",
    "    def pull(self):\n",
    "        \"\"\"Pull the arms\n",
    "        \n",
    "        Returns:\n",
    "            reward (float): reward of the arms\n",
    "        \"\"\"\n",
    "\n",
    "        if self.distribution == 'bernoulli':\n",
    "            return np.random.binomial(1,self.mus)\n",
    "        if self.distribution == 'gaussian':\n",
    "            return np.random.normal(self.mus, self.sigmas)\n",
    "\n",
    "class karmpolicy:\n",
    "    \"\"\"This is the k arm bandit policy\n",
    "\n",
    "    Attributes:\n",
    "        d: number of armsArgs:\n",
    "            index (int): index of the arm to pull\n",
    "        self.muhat (array): empiric mean of the arms\n",
    "        t (int): time step\n",
    "        w (array): number of time an arm is played\n",
    "        regrets (list) : regret at each time step\n",
    "        policy (str): policy to use\n",
    "        params (array): parameters of the policy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, karmbandit, policy, algoparams = None, prior = \"uniform\"):\n",
    "        \"\"\"Init the k arm bandit policy\n",
    "\n",
    "        Args:\n",
    "            d (int): number of arms\n",
    "            policy (str): policy to use\n",
    "            algoparams (array): parameters of the policy, c for ucb, epsilon for epsilon-greedy, param of the prior for thompson sampling\n",
    "            prior (str): by default it is the uniform prior\n",
    "        \"\"\"\n",
    "        # get the parameters of the k arm bandit problem\n",
    "        self.karmbandit = karmbandit\n",
    "        self.d = karmbandit.d\n",
    "        self.bestarm = karmbandit.best_arm\n",
    "        self.mus = karmbandit.mus\n",
    "        self.sigmas = karmbandit.sigmas\n",
    "        self.distribution = karmbandit.distribution\n",
    "        self.mustar = karmbandit.mustar\n",
    "\n",
    "        # store in the class the policy and the initial parameters\n",
    "        self.policy = policy\n",
    "        self.algoparams = algoparams\n",
    "\n",
    "        # initialise the paramater of the algorithm\n",
    "        self.regrets = []\n",
    "        self.muhats = np.zeros(self.d)\n",
    "        self.prior = prior\n",
    "        # small hack to avoid division by 0 before the arm is played for the first time\n",
    "        self.t = 1\n",
    "        self.w = np.ones(self.d) * 10**(-8)\n",
    "\n",
    "        if policy == 'epsilon-greedy':\n",
    "            self.epsilon = self.algoparams[0]\n",
    "        if policy == 'explore-commit':\n",
    "            self.explore = self.algoparams[0]\n",
    "            self.currentexplore = 1\n",
    "        if policy == 'ucb':\n",
    "            self.c = self.algoparams[0]\n",
    "        if policy == 'thompson-sampling':\n",
    "            if self.distribution == 'bernoulli':\n",
    "                if self.prior == \"beta\":\n",
    "                    self.alphas = self.algoparams[:0].copy()\n",
    "                    self.betas = self.algoparams[:1].copy()\n",
    "                if self.prior == \"uniform\":\n",
    "                    self.alphas = np.ones(self.d)\n",
    "                    self.betas = np.ones(self.d)\n",
    "                if self.prior == \"gaussian\":\n",
    "                    # In the case of a gaussian prior for bernoulli distribution we assume that the prior is uniform on R\n",
    "                    # And that the bernoulli are gaussian with variance 1/4 \n",
    "                    self.mus = np.zeros(self.d)\n",
    "                    self.sigmapost = 10**-8 * np.ones(self.d)\n",
    "            if self.distribution == 'gaussian':\n",
    "                if prior == \"gaussian\":\n",
    "                    self.mushats = self.algoparams[:0].copy()\n",
    "                    self.sigmapost = self.algoparams[:1].copy()\n",
    "                if self.prior == \"uniform\":\n",
    "                    self.mushats = np.zeros(self.d)\n",
    "                    self.sigmapost = 10**8 * np.ones(self.d)\n",
    "        if policy == \"klucb\":\n",
    "            pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # initialise the paramater of the algorithm\n",
    "        self.regrets = []\n",
    "        self.muhats = np.zeros(self.d)\n",
    "        # small hack to avoid division by 0 before the arm is played for the first time\n",
    "        self.t = 1\n",
    "        self.w = np.ones(self.d) * 10**(-8)\n",
    "\n",
    "        if self.policy == 'epsilon-greedy':\n",
    "            self.epsilon = self.algoparams[0]\n",
    "        if self.policy == 'explore-commit':\n",
    "            self.explore = self.algoparams[0]\n",
    "            self.currentexplore = 1\n",
    "        if self.policy == 'ucb':\n",
    "            self.c = self.algoparams[0]\n",
    "        if self.policy == 'thompson-sampling':\n",
    "            if self.distribution == 'bernoulli':\n",
    "                if self.prior == \"beta\":\n",
    "                    self.alphas = self.algoparams[:0].copy()\n",
    "                    self.betas = self.algoparams[:1].copy()\n",
    "                if self.prior == \"uniform\":\n",
    "                    self.alphas = np.ones(self.d)\n",
    "                    self.betas = np.ones(self.d)\n",
    "                if self.prior == \"gaussian\":\n",
    "                    # In the case of a gaussian prior for bernoulli distribution we assume that the prior is uniform on R\n",
    "                    # And that the bernoulli are gaussian with variance 1/4 \n",
    "                    self.mus = np.zeros(self.d)\n",
    "                    self.sigmapost = 10**-8 * np.ones(self.d)\n",
    "            if self.distribution == 'gaussian':\n",
    "                if self.prior == \"gaussian\":\n",
    "                    self.mushats = self.algoparams[:0].copy()\n",
    "                    self.sigmapost = self.algoparams[:1].copy()\n",
    "                if self.prior == \"uniform\":\n",
    "                    self.mushats = np.zeros(self.d)\n",
    "                    self.sigmapost = 10**8 * np.ones(self.d)\n",
    "        if self.policy == \"klucb\":\n",
    "            pass\n",
    "    \n",
    "\n",
    "\n",
    "    def select(self):\n",
    "        \"\"\"Select the arm to pull according to the algorithm policy\n",
    "        \n",
    "        Returns:\n",
    "            index (int): index of the arm to pull\n",
    "        \"\"\"\n",
    "        if self.policy == 'ucb':\n",
    "            index = np.argmax(self.muhats + self.c * np.sqrt(2*np.log(self.t) / (self.w)))\n",
    "        if self.policy == 'epsilon-greedy':\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                index = np.random.randint(self.d)\n",
    "            else:\n",
    "                index = np.argmax(self.muhats)\n",
    "\n",
    "        \n",
    "        if self.policy == 'explore-commit':\n",
    "            # keep exploring until every arm are explored self.explore times\n",
    "            if np.any(self.w < self.explore):\n",
    "                index = np.argmax(self.w < self.explore)\n",
    "            else:\n",
    "                index = np.argmax(self.muhats)\n",
    "\n",
    "        if self.policy == 'ucb':\n",
    "            index = np.argmax(self.muhats + self.c * np.sqrt(2*np.log(self.t) / (self.w)))\n",
    "\n",
    "\n",
    "        if self.policy == 'thompson-sampling':\n",
    "            if self.prior == 'bernoulli' or self.prior == 'uniform':\n",
    "                index = np.argmax(np.random.beta(self.alphas, self.betas))\n",
    "            if self.prior == 'gaussian':\n",
    "                index = np.argmax(np.random.normal(self.muhats, self.sigmapost))\n",
    "\n",
    "        if self.policy == \"klucb\":\n",
    "            pass\n",
    "        return index\n",
    "\n",
    "    def update(self, index, reward):\n",
    "        \"\"\"Update the policy\n",
    "        \n",
    "        Args:\n",
    "            index (int): index of the arm to pull\n",
    "            reward (float): reward of the arm\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "\n",
    "        \n",
    "        if self.w[index] < 0.5:\n",
    "            # correct the hack to avoid division by 0\n",
    "            self.w[index] = 1\n",
    "        else:\n",
    "            self.w[index] += 1\n",
    "        if self.policy == 'epsilon-greedy':\n",
    "            self.muhats[index] = (self.muhats[index] * (self.w[index] - 1) + reward) / self.w[index]\n",
    "        if self.policy == 'explore-commit':\n",
    "            self.muhats[index] = (self.muhats[index] * (self.w[index] - 1) + reward) / self.w[index]\n",
    "\n",
    "        if self.policy == \"ucb\":\n",
    "            self.muhats[index] = (self.muhats[index] * (self.w[index] - 1) + reward) / self.w[index]\n",
    "\n",
    "        if self.policy == 'thompson-sampling':\n",
    "            if self.distribution == 'bernoulli':\n",
    "                if self.prior == 'beta' or self.prior == 'uniform':\n",
    "                    self.alphas[index] += reward\n",
    "                    self.betas[index] += 1 - reward\n",
    "                if self.prior == 'gaussian':\n",
    "                    self.muhats[index] = (self.muhats[index] * (self.w[index] - 1) + reward) / self.w[index] \n",
    "                    # I already verified the above formula\n",
    "                    if self.sigmapost[index] == 10**8:\n",
    "                        self.sigmapost[index] = np.sqrt(1/4)\n",
    "                    else:\n",
    "                        self.sigmapost[index] = np.sqrt((1/self.sigmapost[index]**2 + 1/4)**-1)\n",
    "                    \n",
    "\n",
    "            if self.distribution == 'gaussian':\n",
    "                if self.prior == 'uniform':\n",
    "                    self.muhats[index] = (self.muhats[index] * (self.w[index] - 1) + reward) / self.w[index]\n",
    "                    if self.sigmapost[index] == 10**8:\n",
    "                        self.sigmapost[index] = self.sigmas[index]\n",
    "                    else:\n",
    "                        self.sigmapost[index] = np.sqrt((1/self.sigmapost[index]**2 + 1/self.sigmas[index]**2)**-1)\n",
    "                if self.prior == 'gaussian':\n",
    "                    self.sigmapost[index] = np.sqrt((1/self.sigmapost[index] + 1/self.sigmas[index])**-1)\n",
    "                    self.muhats = self.sigmapost[index]**2 * (self.muhats[index] / self.sigmapost[index]**2 + reward / self.sigmas[index]**2)\n",
    "                    # The above formula is verified\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        self.regrets.append(self.mustar - self.mus[index])\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'karmbandit' object has no attribute 'distributions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m problem \u001b[39m=\u001b[39m karmbandit(d, distribution\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbernoulli\u001b[39m\u001b[39m'\u001b[39m, params\u001b[39m=\u001b[39m[\u001b[39m0.1\u001b[39m, \u001b[39m0.2\u001b[39m, \u001b[39m0.3\u001b[39m, \u001b[39m0.4\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.6\u001b[39m, \u001b[39m0.7\u001b[39m, \u001b[39m0.8\u001b[39m, \u001b[39m0.9\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[39m# initialise the algorithms\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m TS \u001b[39m=\u001b[39m karmpolicy(problem, \u001b[39m\"\u001b[39;49m\u001b[39mthompson-sampling\u001b[39;49m\u001b[39m\"\u001b[39;49m, prior \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39muniform\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m UCB \u001b[39m=\u001b[39m karmpolicy(problem, \u001b[39m\"\u001b[39m\u001b[39mucb\u001b[39m\u001b[39m\"\u001b[39m, algoparams\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m])\n\u001b[1;32m     12\u001b[0m egreedy \u001b[39m=\u001b[39m karmpolicy(problem, \u001b[39m\"\u001b[39m\u001b[39mepsilon-greedy\u001b[39m\u001b[39m\"\u001b[39m, algoparams\u001b[39m=\u001b[39m[d\u001b[39m/\u001b[39m(\u001b[39m0.1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m/\u001b[39mT])\n",
      "Cell \u001b[0;32mIn[23], line 74\u001b[0m, in \u001b[0;36mkarmpolicy.__init__\u001b[0;34m(self, karmbandit, policy, algoparams, prior)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmus \u001b[39m=\u001b[39m karmbandit\u001b[39m.\u001b[39mmus\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmas \u001b[39m=\u001b[39m karmbandit\u001b[39m.\u001b[39msigmas\n\u001b[0;32m---> 74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution \u001b[39m=\u001b[39m karmbandit\u001b[39m.\u001b[39;49mdistributions\n\u001b[1;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmustar \u001b[39m=\u001b[39m karmbandit\u001b[39m.\u001b[39mmustar\n\u001b[1;32m     77\u001b[0m \u001b[39m# store in the class the policy and the initial parameters\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'karmbandit' object has no attribute 'distributions'"
     ]
    }
   ],
   "source": [
    "# test the code\n",
    "\n",
    "d=10\n",
    "T = 10000\n",
    "# create a d arm bandit problem\n",
    "problem = karmbandit(d, distribution='bernoulli', params=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "\n",
    "# initialise the algorithms\n",
    "\n",
    "TS = karmpolicy(problem, \"thompson-sampling\", prior = \"uniform\")\n",
    "UCB = karmpolicy(problem, \"ucb\", algoparams=[1])\n",
    "egreedy = karmpolicy(problem, \"epsilon-greedy\", algoparams=[d/(0.1)**2/T])\n",
    "excommit = karmpolicy(problem, \"explore-commit\", algoparams=[np.log(T*(0.1)**2)/(0.1)**2])\n",
    "\n",
    "\n",
    "for i in range(T):\n",
    "\n",
    "    rewards = problem.pull()\n",
    "    index = TS.select()\n",
    "    TS.update(index, rewards[index])\n",
    "\n",
    "    index = UCB.select()\n",
    "    UCB.update(index, rewards[index])\n",
    "    \n",
    "    index = egreedy.select()\n",
    "    egreedy.update(index, rewards[index])\n",
    "\n",
    "    index = excommit.select()\n",
    "    excommit.update(index, rewards[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpy.broadcast at 0x181d650>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast([1],[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([2,3,4]) < 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
